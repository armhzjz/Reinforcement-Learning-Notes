{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an introduction to the MDPs, let's start with an example (which I took from the [Machine Learning Nanodegree from Udacity](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t)).\n",
    "\n",
    "Let's suppose that we have a roboto that collects empty cans of soda without any human intervention and it should decide by itself whether or not he needs to recharge its batteries - which means it has to go to the charging area. We want to define this problem by means of the *MDP*.\n",
    "\n",
    "So, **what are the actions?**\n",
    "\n",
    "    * search for a can\n",
    "    * recharge its battery\n",
    "    * wait for a can to be collected\n",
    "    \n",
    "We call the set of all possible actions that an agent can take within the context of this problem (that is, the available actions to the agent), the **Action Space**. The action space is denoted by $A$.\n",
    "\n",
    "So, **what are the states?**\n",
    "\n",
    "    * battery high\n",
    "    * battery low\n",
    "    \n",
    "The **State Space** is denoted as $S$ for continuing tasks, and as $S^+$ for episodic tasks (in which case $S^+$ contains all the possible states **except** the terminal states).\n",
    "\n",
    "In case there are some states where only a subset of the actions are available, we can also use $A(s)$ to refer to the set of actions available in state $s \\in S$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a diagram that shows the state diagram with probability of a transition to occur (blue numbers) and the reward (in number of empty soda cans) that transition brings (orange numbers).\n",
    "\n",
    "![MDP diagram](images/markovDiagram.png)\n",
    "\n",
    "At an arbitrary time step $t$, the agent-environment interaction has evolved as a sequence of states, actions, and rewards\n",
    "\n",
    "($S_0$, $A_0$, $R_1$ ,$S_1$, $A_1$, \\.\\.\\., $R_{t−1}$, $S_{t−1}$, $A_{t−1}$, $R_t$, $S_t$, $A_t$)\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, it considers only the state and action at the previous time step ($S_t$, $A_t$).\n",
    "\n",
    "In particular, it does not care what state was presented to the agent more than one step prior. (In other words, the environment does not consider any of $\\{S_0, $\\.\\.\\.$, S_{t−1}\\}$.)\n",
    "\n",
    "And, it does not look at the actions that the agent took prior to the last one. (In other words, the environment does not consider any of $\\{A_0, $\\.\\.\\.$, A_{t−1}\\}$).\n",
    "\n",
    "Furthermore, how well the agent is doing, or how much reward it is collecting, has no effect on how the environment chooses to respond to the agent. (In other words, the environment does not consider any of $\\{R_0, $\\.\\.\\.$, R_t\\}$).\n",
    "\n",
    "Because of this, we can completely define how the environment decides the state and reward by specifying\n",
    "\n",
    " $$p(s', r \\mid s,a) \\doteq \\mathbb{P}(S_{t+1} = s', R_{t+1} = r \\mid S_t = s, A_t = a)$$\n",
    "\n",
    "for each possible $s', r, s,$ and $a$. These conditional probabilities are said to specify the **one-step dynamics** of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision Process (MDP); the definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (finite) Markov Decision Process (MDP) is defined by:\n",
    "\n",
    "   * a (finite) set of states $\\mathcal{S}$\n",
    "   * a (finite) set of actions $\\mathcal{A}$\n",
    "   * a (finite) set of rewards $\\mathcal{R}$\n",
    "   * the one-step dynamics of the environment:\n",
    "   $$p(s', r \\mid s,a) \\doteq \\mathbb{P}(S_{t+1} = s', R_{t+1} = r \\mid S_t = s, A_t = a)$$\n",
    "   for all s, s', a and r\n",
    "   * a discount rate $\\gamma \\in [0, 1]$\n",
    "   \n",
    "*Hint:* The discount rate will have to be different than 0, but maybe close to 1 to avoid that the agent becomes too short-sighted to a fault. An example to remember discount rate; let us define a discount rate as $\\gamma = 0.9$. Our discounted return would then be:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...$$ and continues without limit.\n",
    "\n",
    "![Markov Decission Process Definition](images/MDPDefinition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Notebook 1 & 2 (Reinforcement Learning Framework & Markov Decision Processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reinforcement Learning Framework](images/ReinforcementLearningFramework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Setting, Revisited\n",
    "\n",
    "* The reinforcement learning (RL) framework is characterized by an **agent** learning to interact with its **environment**.\n",
    "* At each time step, the agent receives the environment's **state** (the environment presents a situation to the agent), and the agent must choose an appropriate **action** in response. One time step later, the agent receives a **reward** (the environment indicates whether the agent has responded appropriately to the state) and a new **state**.  \n",
    "* All agents have the goal to maximize expected **cumulative reward**, or the expected sum of rewards attained over all time steps.\n",
    "\n",
    "### Episodic vs. Continuing Tasks\n",
    "\n",
    "* A **task** is an instance of the reinforcement learning (RL) problem.\n",
    "* **Continuing tasks** are tasks that continue forever, without end.\n",
    "* **Episodic tasks** are tasks with a well-defined starting and ending point.\n",
    "    * In this case, we refer to a complete sequence of interaction, from start to finish, as an **episode**.\n",
    "    * Episodic tasks come to an end whenever the agent reaches a **terminal state**.\n",
    "\n",
    "### The Reward Hypothesis\n",
    "\n",
    "* **Reward Hypothesis**: All goals can be framed as the maximization of (expected) cumulative reward.\n",
    "\n",
    "### Goals and Rewards\n",
    "\n",
    "* (Please see Part 1 and Part 2 to review an example of how to specify the reward signal in a real-world problem.)\n",
    "\n",
    "### Cumulative Reward\n",
    "\n",
    "* The **return at time step** $t$ is $G_t := R_{t+1} + R_{t+2} + R_{t+3} + $\\.\\.\\.\n",
    "* The agent selects actions with the goal of maximizing expected (discounted) return. (Note: discounting is covered in the next concept.)\n",
    "\n",
    "### Discounted Return\n",
    "\n",
    "* The **discounted return at time step** $t$ is $G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$\n",
    "* The **discount rate** $\\gamma$ is something that you set, to refine the goal that you have the agent.\n",
    "    * It must satisfy $0 \\leq \\gamma \\leq 1$\n",
    "    * If $\\gamma=0$, the agent only cares about the most immediate reward.\n",
    "    * If $\\gamma=1$, the return is not discounted.\n",
    "    * For larger values of $\\gamma$, the agent cares more about the distant future. Smaller values of $\\gamma$ result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.\n",
    "\n",
    "### MDPs and One-Step Dynamics\n",
    "\n",
    "* The **state space** $\\mathcal{S}$ is the set of all (nonterminal) states.\n",
    "* In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of all states, including terminal states.\n",
    "* The action space $\\mathcal{A}$ is the set of possible actions. (Alternatively, $\\mathcal{A}(s)$ refers to the set of possible actions available in state $s \\in \\mathcal{S}$.)\n",
    "* (Please see Part 2 to review how to specify the reward signal in the recycling robot example.)\n",
    "* The **one-step dynamics** of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying $p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a)$ for each possible $s', r, s, \\text{and } a$.\n",
    "* A **(finite) Markov Decision Process (MDP)** is defined by:\n",
    "    * a (finite) set of states $\\mathcal{S}$ (or $\\mathcal{S}^+$, in the case of an episodic task)\n",
    "    * a (finite) set of actions $\\mathcal{A}$\n",
    "    * a set of rewards $\\mathcal{R}$\n",
    "    * the one-step dynamics of the environment\n",
    "    * the discount rate $\\gamma \\in [0,1]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
