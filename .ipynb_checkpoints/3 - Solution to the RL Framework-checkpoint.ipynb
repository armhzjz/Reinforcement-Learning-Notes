{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the RL Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the solution as a series of actions to be taken by the agent. But the correct action will change according to the current situation, so in different situation a different set of actions may be taken.\n",
    "\n",
    "Reward is always decided in the context of the state that it was decided, along with the state that follows.\n",
    "\n",
    "So, if the agent learns the appropriate response to any environment state (that is, if the agent knows what actions to take according the the different situations it faces), then it can be said that we have a solution to our problem. This motivates the idea of a policy.\n",
    "\n",
    "A policy can be thought of as a maping from the set of environment states to the set of possible actions.\n",
    "\n",
    "$$\\pi : \\mathbb{S} \\longrightarrow \\mathbb{A}$$\n",
    "\n",
    "We can think of this as:\n",
    "\n",
    "state (as input) $\\longrightarrow$ factory ($\\pi$) $\\longrightarrow$ action (as an output)\n",
    "\n",
    "If the agent wants to keep track of its strategy, all it has to do, is to build this factory (i.e. $\\pi$) or to specify thsi mapping.\n",
    "\n",
    "Policies can be *deterministic* or *stochastic*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deterministic Policy - definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **deterministic policy** is a mapping $\\pi : \\mathbb{S} \\longrightarrow \\mathbb{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Policy - definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **stochastic policy** is a mapping $\\pi : \\mathbb{S} x \\mathbb{A} \\longrightarrow [0, 1]$.$$\\pi(a \\mid s) = \\mathbb{P}(A_t = a \\mid S_t = s)$$\n",
    "\n",
    "state (as input) $\\longrightarrow$ factory ($\\pi$) $\\longrightarrow$ probability that the agent takes action $a$ while in state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a Deterministic Policy:\n",
    "\n",
    "$\\pi(low) = recharge$\n",
    "\n",
    "$\\pi(high) = search$\n",
    "\n",
    "#### Example of a Stochastic Policy:\n",
    "\n",
    "$\\pi(recharge \\mid low) = 0.5$\n",
    "\n",
    "$\\pi(wait \\mid low) = 0.4$\n",
    "\n",
    "$\\pi(search \\mid low) = 0.1$\n",
    "\n",
    "$\\pi(search \\mid high) = 0.9$\n",
    "\n",
    "$\\pi(wait \\mid high) = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Value Function - formal definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For each state $S$, it yields the expected discounted return if the agent starts in state $S$ and then uses the policy to choose its actions for all time steps.\"\n",
    "\n",
    "The state value function will always correspond to a particular policy $\\pi$. So if we change the policy, then we change the state value functions as well. The state value function is tipically denoted as $v_{\\pi}$.\n",
    "\n",
    "The value of state $s$ under a policy $\\pi$ is:\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]$$\n",
    "\n",
    "![](images/StateValueFunction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a state $S_t$ (any state) is normally expressed as the sumatory of the immediate reward plus the value of the state that follows (i.e. $S_{t+1}$). So we can say that the value of a state is simply the *expected discounted reward* - the state values are/may be affected by a discount rate.\n",
    "\n",
    "![Expected Discounted Reward](images/StateValueImage.png)\n",
    "\n",
    "\n",
    "So, to be more precise we can say that **the value of any state can be expressed as the sum of the immediate reward plus the *discounted value* of the state that follows**.\n",
    "\n",
    "This is expressed with the *Bellman Expectation Equation*:\n",
    "\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s]$$\n",
    "\n",
    "The value of any state $s$ is equal to the expected value of the immediate reward plus the discounted value of the state that follows, if the agent starts in state $S$ and then uses the policy $\\pi$ to choose its actions for all the time steps.\n",
    "\n",
    "![Bellman Expectation Equation](images/BellmanExpectationEquation.png)\n",
    "\n",
    "**The value is Expected (i.e. expected value)** becuase for more complicated environments, the immediate reward and next state cannot be known with certainty.  \n",
    "The main idea is that *we can express the value of any state in the MDP in terms of the immediate reward and discounted value of the state that follows*.\n",
    "\n",
    "### Calculating the Expectation\n",
    "\n",
    "In the event that the agent's policy $\\pi$ is **deterministic**, the agent selects action $\\pi(s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over two variables ($s'$ and $r$):\n",
    "$$v_\\pi(s) = \\sum_{s´\\in S⁺, r \\in R} p(s´,r \\mid s, \\pi(s))(r + \\gamma v_\\pi(s´))$$\n",
    "\n",
    "In this case, we multiply the sum of the reward and discounted value of the next state $(r + \\gamma v_\\pi(s´))$ by its corresponding probability $p(s´,r\\mid s,\\pi(s))$ and sum over all possibilities to yield the expected value.\n",
    "\n",
    "If the agent's policy $\\pi$ is **stochastic**, the agent selects actions $a$ with probability $\\pi(a\\mid s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over three variables ($s´$, $r$ and $a$):\n",
    "$$v_\\pi(s) = \\sum_{s´\\in S⁺, r \\in R, a\\in A(s)} \\pi(a\\mid s)p(s´,r \\mid s, a)(r + \\gamma v_\\pi(s´))$$\n",
    "\n",
    "In this case, we multiply the sum of the reward and discounted value of the next state $(r + \\gamma v_\\pi(s´))$ by its corresponding probability $\\pi(a\\mid s)p(s´,r\\mid s,a)$ and sum over all possibilities to yield the expected value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy $\\pi'$ is better or equal to the policy $\\pi$ if its state value function is greater than or equal to the policy $\\pi$ for all states.\n",
    "\n",
    "Points for consideration:\n",
    " * if we have any 2 policies we might not be able to decide which of them is better - it could be that the 2 policies are not comparable\n",
    " * it will always exist one policy which will be better than or equal to other policies - this would be the **optimal policy**.\n",
    " \n",
    "Formal definition:\n",
    "\n",
    "$\\pi' \\geq \\pi$ *if and only if* $v_{\\pi'}(s) \\geq v_{\\pi}(s)$ for all $s \\in S$\n",
    "\n",
    "An optimal policy $\\pi_*$ satisfies $\\pi_* \\geq \\pi$ for all $\\pi$\n",
    "\n",
    "*Hint*: An optimal policy is guaranteed to exist, but may not be unique\n",
    "\n",
    "*Hint*: The optimal state-value function is denoted $v_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do we determine an optimal policy for a complicated MDP?*\n",
    "\n",
    "We call $q_\\pi$ the **action-value function for policy $\\pi$**.\n",
    "\n",
    "The value of taking action $a$ in state $s$ under a policy $\\pi$ is:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "In words: For each state $s$ and action $a$, it yields the expected return if the agent starts in state $s$, then chooses action $a$ and then uses the policy $\\pi$ to choose its actions for all time steps.\n",
    "![Action-Value function definition](images/action-value-function-definition.png)\n",
    "\n",
    "For comparison, the state value function yields the expected return for each state, if the agent starts in that state, and then follows the policy for all time steps, whereas the **action-value function** yields the expected return, if the agent starts in that state, takes the action, and then follows the policy for all future time steps.  \n",
    "Action-Value functions will keep track of the value returned by **each one** of the possible actions when in a state.\n",
    "![Comparing State-Value and Action-Value](images/comparisonStateValueVSActionValue.png)\n",
    "### \n",
    "As an example, the picture below depicts the calculation of the Action-Value of state $s1$ (the state our agent starts at) and chosen action \"down\":\n",
    "![Action-Value function example](images/action-value-function-example.png)\n",
    "\n",
    "If we make the calculation of every action that makes sense, for every state, we will end up with the following Action-Values:\n",
    "![](images/action-values-calculations-results.png)\n",
    "\n",
    "### Revisiting some of the previous definitions\n",
    "![](images/revisitingDefinitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True or False:\n",
    "For a deterministic policy $\\pi$,\n",
    "$$v_\\pi(s) = q_\\pi(s, \\pi(s))$$ holds for all $s \\in S$?\n",
    "\n",
    "**True**\n",
    "\n",
    "It depends on how we have defined the action-value function. The value of the state-action pair $s, \\pi(s)$ is the expected return if the agent starts in state $s$, takes action $\\pi(s)$ and then follows the policy $\\pi$. In other words, it is the expected return if the agent starts in state $s$ and then follows the policy $\\pi$, which is exactly equal to the value of state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy may be estimated by the agent $\\longrightarrow$ The agent interacts with the environment and from this interacton, it estimates the optimal action value function. Then the agent uses that value function to get the optimal policy.\n",
    "\n",
    "Interaction $\\longrightarrow q_* \\longrightarrow \\pi_*$ \n",
    "\n",
    "Assuming the agent has already the action-value function and that it still does not have the optimal policy, then for each state, the agent picks the action that yields the highest expected return.\n",
    "\n",
    "If the state space $S$ and action space $A$ are finite, we can represent the optimal Action-Value function $q_*$ in a table, where we have one entry for each possible environment state $s \\in S^+$ and action $a \\in A$.\n",
    "The value for a particular state-action pair ($s, a$) is the expected return if the agent starts in state $s$, takes action $a$, and then follows the optimal policy $\\pi_*$.\n",
    "\n",
    "The table below is populated with some values for a hypothetical Markov decision process - where $\\mathbb{S} = \\{s_1, s_2, s_3\\}$ and $\\mathbb{A} = \\{a_1, a_2, a_3\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| _ |$a_1$ | $a_2$ | $a_3$ |\n",
    "|------|------|------|------|\n",
    "| $s_1$  | 1 | 2 | -3|\n",
    "| $s_2$  | -2 | 1 | 3|\n",
    "| $s_3$  | 4 | 4 | -5|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an agent has determined the optimal aciton-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting:\n",
    "\n",
    "$$\\pi_*(s) = arg max_{a \\in A(s)}q_*(s, a)$$\n",
    "for all $s \\in S$.\n",
    "\n",
    "Note, that for this to be true, it must hold also true that:\n",
    "\n",
    "$$v_*(s) = max_{a \\in A(s)}q_*(s, a)$$\n",
    "\n",
    "In the event that there is some state $s \\in S$ for which multiple actions $a \\in A(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the maximizing actions. It must be only ensure that the actions that do not maximize the action-value function for a particular state are given 0% probability under the policy.\n",
    "\n",
    "Therefore, the optimal policy $\\pi_*$ for the corresponding MDP must satisfy:\n",
    " * $\\pi_*(s_1) = a_2$, or equivalently, $\\pi_*(a_2 \\mid s_1) = 1$\n",
    " * $\\pi_*(s_2) = a_3$, or equivalently, $\\pi_*(a_3 \\mid s_2) = 1$\n",
    " \n",
    "Why? Because:\n",
    " * $a_2 = arg max_{a \\in A(s_1)}q_*(s_1, a)$, and\n",
    " * $a_3 = arg max_{a \\in A(s_2)}q_*(s_2, a)$\n",
    " \n",
    " \n",
    " In other words, under the optimal policy, the agent must choose action $a_2$ when in state $s_1$, and it must choose action $a_3$ when in state $s_2$.\n",
    " \n",
    " Note that for state $s_3$, $a_1, a_2 \\in argmax_{a \\in A(s_3)}q_*(s_3, a)$ and therefore the agent can choose either action $a_1$ or $a_2$ under the optimal policy, but it can never choose acton $a_3$. So, the optimal policy $\\pi_*$ must satisfy:\n",
    "  * $\\pi_*(a_1 \\mid s_3) = p$,\n",
    "  * $\\pi_*(a_2 \\mid s_3) = q$,\n",
    "  * $\\pi_*(a_3 \\mid s_3) = 0$\n",
    "  \n",
    "where $p, q \\geq 0$, and $p + q = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "![](images/RLFrameworkSolutionSummary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
